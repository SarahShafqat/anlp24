{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n53a5KC4BnKz"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp24/blob/main/1.words/EvaluateTokenizationForSentiment.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8AD2hDbBnK5"
      },
      "source": [
        "This notebook evaluates different methods for tokenization and stemming/lemmatization\n",
        "and assesses the impact on binary sentiment classification, using a train/dev dataset of sample of 1000 reviews from the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/).  Each tokenization method is evaluated on the same learning algorithm ($\\ell_2$-regularized logistic regression); the only difference is the tokenization process. For more, see: http://sentiment.christopherpotts.net/tokenizing.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download code and data\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/main/1.words/happyfuntokenizing.py\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/main/data/sentiment.1000.train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp24/main/data/sentiment.1000.dev.txt"
      ],
      "metadata": {
        "id": "jj0huAdwBs0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOu0wKUqBnK6"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import spacy\n",
        "from nltk.stem.porter import *\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn import linear_model\n",
        "from happyfuntokenizing import Tokenizer as potts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizationTest():\n",
        "\n",
        "\tdef __init__(self, trainFile, devFile):\n",
        "\t\tself.trainFile=trainFile\n",
        "\t\tself.devFile=devFile\n",
        "\n",
        "\tdef read_data(self, filename, tokenizer):\n",
        "\t\tcorpus=[]\n",
        "\t\tY=[]\n",
        "\t\twith open(filename, encoding=\"utf-8\") as file:\n",
        "\t\t\tfor idx,line in enumerate(file):\n",
        "\t\t\t\tcols=line.rstrip().split(\"\\t\")\n",
        "\t\t\t\tlabel=cols[0]\n",
        "\t\t\t\ttext=cols[1]\n",
        "\t\t\t\ttokens=' '.join(tokenizer(text))\n",
        "\t\t\t\tcorpus.append(tokens)\n",
        "\t\t\t\tY.append(label)\n",
        "\n",
        "\t\treturn corpus, Y\n",
        "\n",
        "\tdef evaluate(self, tokenizer):\n",
        "\n",
        "\t\ttrain_corpus, train_labels=self.read_data(self.trainFile, tokenizer)\n",
        "\t\tdev_corpus, dev_labels=self.read_data(self.devFile, tokenizer)\n",
        "\n",
        "\t\tvectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
        "\t\tX_train = vectorizer.fit_transform(train_corpus)\n",
        "\t\tX_dev = vectorizer.transform(dev_corpus)\n",
        "\n",
        "\t\tle = preprocessing.LabelEncoder()\n",
        "\t\tle.fit(train_labels)\n",
        "\n",
        "\t\tY_train=le.transform(train_labels)\n",
        "\t\tY_dev=le.transform(dev_labels)\n",
        "\n",
        "\t\tlogreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2')\n",
        "\t\tlogreg.fit(X_train, Y_train)\n",
        "\t\tprint(\"Function '%s' Accuracy: %.3f\" % (tokenizer.__name__, logreg.score(X_dev, Y_dev)))"
      ],
      "metadata": {
        "id": "HY_V8LKmBoo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo6y6M08BnK7"
      },
      "outputs": [],
      "source": [
        "# spaCy lemmatization needs tagger but disable the rest\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner,parser'])\n",
        "nlp.remove_pipe('ner')\n",
        "nlp.remove_pipe('parser')\n",
        "\n",
        "# load NLTK porter stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# load Potts sentiment tokenizer\n",
        "potts_tokenizer=potts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xxgxawHBnK7"
      },
      "outputs": [],
      "source": [
        "def spacy_tokenizer(data):\n",
        "    spacy_tokens=nlp(data)\n",
        "    return [token.text for token in spacy_tokens]\n",
        "\n",
        "def spacy_lemmatizer(data):\n",
        "    spacy_tokens=nlp(data)\n",
        "    return [token.lemma_ for token in spacy_tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw5uOlNXBnK8"
      },
      "outputs": [],
      "source": [
        "tester=TokenizationTest(\"sentiment.1000.train.txt\", \"sentiment.1000.dev.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3pqLuFlBnK8"
      },
      "outputs": [],
      "source": [
        "tester.evaluate(str.split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7fM86jgBnK9"
      },
      "outputs": [],
      "source": [
        "tester.evaluate(stemmer.stem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbGK8OqoBnK9"
      },
      "outputs": [],
      "source": [
        "tester.evaluate(nltk.word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_5QIVUcdBnK-"
      },
      "outputs": [],
      "source": [
        "tester.evaluate(spacy_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "MjaRF32LBnK-"
      },
      "outputs": [],
      "source": [
        "tester.evaluate(spacy_lemmatizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGGoA0TiBnK-"
      },
      "outputs": [],
      "source": [
        "tester.evaluate(potts_tokenizer.tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knwVx8DlBnK-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}